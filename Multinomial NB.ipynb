{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removing stop words before calculating TF-IDF using scikitlearn's set of stop words\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "myStopWords = text.ENGLISH_STOP_WORDS.union(set(stopwords.words('english')))\n",
    "\n",
    "print(myStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# without lemmatization\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(max_df=0.85, stop_words=myStopWords)\n",
    "# vectors =gmail.com vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with lemmatization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in CountVectorizer().build_tokenizer()(doc)]\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), max_df=0.85, stop_words=myStopWords, ngram_range=(1, 2))\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "vectors = transformer.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # from pandas import DataFrame\n",
    "# # print(DataFrame(vectors.A, columns=vectorizer.get_feature_names()).to_string())\n",
    "\n",
    "# # print(vectorizer.vocabulary_)\n",
    "# # print(len(vectorizer.vocabulary_))\n",
    "# cnt = 0\n",
    "# for k,v in vectorizer.vocabulary_.items():\n",
    "#     print(k+\": \"+str(v))\n",
    "#     cnt += 1\n",
    "#     if cnt == 100:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# without lemmatization\n",
    "#vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# with lemmatization\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "vectors_test = transformer.transform(vectors_test)\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-15:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "        print()\n",
    "show_top10(clf, vectorizer, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using gensim LDA for topic modelling\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "gensimCorpus = Sparse2Corpus(vectors)\n",
    "id2word_newsgroups = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "lda = LdaModel(corpus=gensimCorpus, num_topics=20, id2word=id2word_newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda.print_topics(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
