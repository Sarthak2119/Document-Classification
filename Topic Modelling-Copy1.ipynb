{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing stop words before calculating TF-IDF using scikitlearn's set of stop words\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "myStopWords = text.ENGLISH_STOP_WORDS.union(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset =  fetch_20newsgroups(shuffle=True,subset='all', random_state=1,remove=('headers', 'footers', 'quotes'))\n",
    "documents=dataset.data\n",
    "categories = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18846\n"
     ]
    }
   ],
   "source": [
    "print (documents[0])\n",
    "print (len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 5000\n",
    "# with lemmatization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in CountVectorizer().build_tokenizer()(doc)]\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), max_df=0.85, max_features=n_features, stop_words=myStopWords)\n",
    "vectorizedData = vectorizer.fit_transform(documents)\n",
    "transformer = TfidfTransformer()\n",
    "transformedData = transformer.fit_transform(vectorizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NMF is able to use tf-idf\n",
    "tfidf = transformedData\n",
    "tfidf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf = vectorizedData\n",
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='batch', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_words = 15\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print()\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vectorizer = CountVectorizer(vocabulary=tf_feature_names, tokenizer=LemmaTokenizer(), stop_words=myStopWords)\n",
    "test_vectors = test_vectorizer.fit_transform(documents)\n",
    "predict = lda.transform(test_vectors)\n",
    "print(predict.shape)\n",
    "\n",
    "for i in range(5):\n",
    "    print(predict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#method 1 \n",
    "size_training= len(documents)\n",
    "train_output=[]\n",
    "# print (train_output[0])\n",
    "sz=0\n",
    "for i in categories:\n",
    "    g_list=[0]*20\n",
    "    g_list[i]=1\n",
    "    train_output.append(g_list)\n",
    "    \n",
    "print (train_output[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset =  fetch_20newsgroups(shuffle=True,subset='test', random_state=1,remove=('headers', 'footers', 'quotes'))\n",
    "test_doc=test_dataset.data\n",
    "test_cat= test_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vectorizedData = vectorizer.fit_transform(test_doc)\n",
    "test_transformedData = transformer.fit_transform(vectorizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tfidf = test_transformedData\n",
    "tfidf_feature_names = vectorizer.get_feature_names()\n",
    "test_tf = test_vectorizedData\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(test_tfidf)\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='batch', learning_offset=50.,random_state=0).fit(test_tf)\n",
    "no_top_words = 15\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "#print()\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n",
    "test_vectorizer = CountVectorizer(vocabulary=tf_feature_names, tokenizer=LemmaTokenizer(), stop_words=myStopWords)\n",
    "test_vectors = test_vectorizer.fit_transform(test_doc)\n",
    "test_predict = lda.transform(test_vectors)\n",
    "#print(predict.shape)\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(predict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-93429f90ff24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print (train_output[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_cat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mg_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_cat' is not defined"
     ]
    }
   ],
   "source": [
    "#method 1 \n",
    "test_output=[]\n",
    "# print (train_output[0])\n",
    "sz=0\n",
    "for i in test_cat:\n",
    "    g_list=[0]*20\n",
    "    g_list[i]=1\n",
    "    test_output.append(g_list)\n",
    "    \n",
    "print (test_output[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# time to train dataset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "print (gnb.fit(predict,train_output).score(test_predict,test_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
